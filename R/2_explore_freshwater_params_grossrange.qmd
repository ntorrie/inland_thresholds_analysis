---
title: "Flagging Inland Data"
format:
  html:
    toc: true
    toc-depth: 3
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(data.table)
library(qaqcmar)
library(here)
library(ggplot2)
library(sensorstrings)
library(summaryplots)
library(plotly)
library(DT)
library(leaflet)

#Load CMAR river data
dat_all <- readRDS(here("data/2024-09-05_inland_data_prelim_qc.rds"))

dat_clean <- readRDS(here("data/2024-09-05_inland_data_prelim_qc_clean.rds"))

dat_long <- ss_pivot_longer(dat_clean)

st_locations <- dat_all %>%
  distinct(latitude, .keep_all = TRUE)

list <- unique(dat_all$station)

list2 <- unique(dat_clean$station)

dt_options <- list(
  dom = 'ft',
  paging = FALSE,
  searching = TRUE,
  scrollY = "500px",
  scrollX = "500px",
  columnDefs = list(list(className = 'dt-center', targets = "_all"))
)


```


# Inland Parameters: Explore Thresholds

## Summary

CMAR has collected data on several inland bodies of freshwater in Nova Scotia through the wild Atlantic salmon river monitoring project. In addition, CMAR has data collected on several freshwater lakes (e.g. Piper Lake), which is currently published through the Water Quality branch of the [Coastal Monitoring Program](https://cmar.ca/coastal-monitoring-program/#station). 

CMAR intends to process and republish all inland data under a new "Inland" branch of the Coastal Monitoring Program. Data will be processed in a similar manner to the water quality data, and data flags will be applied using the [qaqcmar](https://github.com/dempsey-CMAR/qaqcmar) package. 

It is suspected that sensors on some rivers were out of the water for some period of time during the deployment. Data flagging efforts will flag data for periods of time sensors were suspected to be exposed. During the periods in which sensors were exposed to air, recorded temperatures fluctuate more quickly than when sensors are submerged. 

The purpose of this document is to help CMAR determine appropriate data flagging tests and thresholds for freshwater (inland) data. We do not currently have enough freshwater data to conduct as thorough an analysis as was done on the saltwater water quality data to develop tests and thresholds, so thresholds may be picked in more subjective ways.



##### Waterbodies in dataset:

```{r, echo = FALSE}
unique(dat_all$waterbody)
```

##### Stations in dataset:

```{r, echo = FALSE}
unique(dat_all$station)
```

##### Stations which may have experienced air exposure:

 - Liscomb 1
 - Liscomb 2
 - LaHave 2
 - Mersey 2
 - Tusket 3
 - Possibly Musquodoboit 1 and 2

# Station Locations

Approximate location of stations included in analysis.

```{r}
#| fig-height: 6

# interactive map
leaflet(st_locations) %>% 
  addProviderTiles(providers$CartoDB.Positron) %>% 
  addCircleMarkers(
    data = st_locations,
    lng = ~longitude, lat = ~latitude, label = ~station,
    weight = 1,
    color = "black",
    fillOpacity = 0.75,
    radius = 5
  ) %>% 
  addScaleBar(
    position = "bottomleft", 
    options = scaleBarOptions(imperial = FALSE)
  ) %>%
  addProviderTiles("Esri.WorldTopoMap")
  #addProviderTiles(providers$CartoDB.Positron)

```


## Plot all Station Data

::: panel-tabset
```{r}
#| warning: false
#| message: false
#| results: asis
#| fig-height: 5

for (i in list) {
  dat_i <- dat_all %>%
    filter(station == i)
  
  cat(paste("### ", i, "\n \n"))
  
  print(
    ggplot(
      dat_i,
      aes(x = timestamp_utc, y = temperature_degree_c, col = station)
    ) +
      geom_line(color = "black", linewidth = 0.75) +
      facet_grid(vars(station)) +
      ggtitle(i)
  )
  
  cat("\n\n")
}
```
:::

## Plot Cleaned Station Data

Suspected outliers have been removed from the following datasets:

 - Liscomb 1
 - Liscomb 2
 - LaHave 2
 - Mersey 2
 - Tusket 3

::: panel-tabset
```{r}
#| warning: false
#| message: false
#| results: asis
#| fig-height: 5

for (i in list2) {
  dat_i <- dat_clean %>%
    filter(station == i)
  
  cat(paste("### ", i, "\n \n"))
  
  print(
    ggplot(
      dat_i,
      aes(x = timestamp_utc, y = temperature_degree_c, col = station)
    ) +
      geom_point(color = "black") +
      facet_grid(vars(station)) +
     # facet_wrap(vars(station), ncol = 4) +
      ggtitle(i)
  )
  cat("\n\n")
}
```
:::

## Distribution of Observations

```{r, echo = FALSE, warning = FALSE, fig.height = 15}
p_i <- dat_clean %>%
  plot_histogram(hist_col = "temperature_degree_c", binwidth = 1) +
  scale_x_continuous("temperature_degree_c") +
  facet_wrap( ~ station, ncol = 2)

p_i

```

## Summary Stats
### By Station
```{r, echo = FALSE, warning = FALSE, message = FALSE}
stats <- dat_long %>% 
  group_by(station, variable) %>% 
  summarise(
    n = n(),
    mean = round(mean(value), digits = 2),
    min = min(value),
    max = max(value),
    stdev = round(sd(value), digits = 2), 
    q_90 = round(quantile(value, probs = 0.90), digits = 2),
    q_95 = round(quantile(value, probs = 0.95), digits = 2),
    q_997 = round(quantile(value, probs = 0.997), digits = 2)
  ) %>%
  ungroup() %>% 
  mutate(n_percent = round(n * 100 / sum(n), digits = 1)) %>% 
  select(station, variable, n, n_percent, everything())


stats %>%
  datatable(rownames = FALSE, options = dt_options)
```

### Pooled
```{r, echo = FALSE, warning = FALSE, message = FALSE}
stats_all <- dat_long %>% 
  summarise(
    n = n(),
    mean = round(mean(value), digits = 2),
    min = min(value),
    max = max(value),
    stdev = round(sd(value), digits = 2), 
    q_90 = round(quantile(value, probs = 0.90), digits = 2),
    q_95 = round(quantile(value, probs = 0.95), digits = 2),
    q_997 = round(quantile(value, probs = 0.997), digits = 2)
  ) %>%
  mutate(n_percent = round(n * 100 / sum(n), digits = 1))  


stats_all %>%
  datatable(rownames = FALSE, options = dt_options)
```

## Mean and Standard Deviation

```{r}
#| warning: false
#| message: false
#| results: asis
#| fig-height: 8

p <- ggplot(
      stats,
      aes(mean, reorder(station, mean), col = mean,
          text = paste("station: ", station, "\nmean: ", mean))
      ) +
      scale_y_discrete(name = "")

p + geom_point(size = 4) +
    geom_errorbar(
      aes(xmin = mean - stdev, xmax = mean + stdev), width = 0
    ) +
    scale_x_continuous("mean +/- standard deviation") +
    theme(
      text = element_text(size = 14),
      legend.position = "none"
    )

```

## Quantiles

```{r}
#| warning: false
#| message: false
#| results: asis
#| fig-height: 8

stats_quantile <- stats %>%
  select(station, contains("q")) %>%
  pivot_longer(cols = contains("q"), names_to = "quantile")

p <- ggplot(stats_quantile,
            aes(quantile, value, group = station, col = station)) +
  geom_point() +
  geom_line() 

p

```


# Apply thresholds

Since the datasets are all normally distributed, mean + SD will be used to develop thresholds. Stations have similar mean + SD, so data has been pooled to determine one threshold to be used to flag all inland river datasets.

## First Attempt 

```{r}
#| warning: false
#| message: false
#| results: asis
#| fig-height: 8

# library(zoo)
# period_hours <- 24
# max_interval_hours <- 2
# align_window <- "center"
# 
# dat_all_roll <- dat_long %>%
#   group_by(county,
#            station,
#            deployment_range,
#            sensor_serial_number,
#            variable) %>%
#   dplyr::arrange(timestamp_utc, .by_group = TRUE) %>%
#   mutate(
#     # sample interval
#     int_sample = difftime(timestamp_utc, lag(timestamp_utc), units = "mins"),
#     int_sample = round(as.numeric(int_sample)),
#     
#     # number of samples in  period_hours
#     # 60 mins / hour * 1 sample / int_sample mins * 24 hours / period
#     n_sample = round((60 / int_sample) * period_hours),
#     # n_sample = if_else(is.na(n_sample), 1, n_sample), # first obs
#     n_sample_effective = case_when(
#       # first observation of each group is NA, which will give error in rollapply
#       is.na(n_sample) ~ 0,
#       # if the sample interval is greater than acceptable limit, set n_sample to 0
#       # so that roll_sd will be NA
#       int_sample > max_interval_hours * 60 ~ 0,
#       TRUE ~ n_sample
#     ),
#     # rolling sd
#     sd_roll = rollapply(
#       value,
#       width = n_sample_effective,
#       align = align_window,
#       FUN = sd,
#       fill = NA
#     ),
#     sd_roll = round(sd_roll, digits = 2)
#   )
# 
# 
# #look at histograms of SD roll colums
# 
# 
# 
# thresh <- qc_calculate_rolling_sd_thresholds(dat_all_roll, temperature_degree_c, stat = "mean_sd", n_sd = 3)

threshold_value <- stats_all$stdev

newrow_sd <- list(qc_test = "rolling_sd",
                  variable = "temperature_degree_c",
                  county = "river_data",
                  month = "NA",
                  sensor_type = "NA",
                  threshold = "rolling_sd_max",
                  threshold_value = threshold_value)

thresholds <- thresholds %>% 
  filter(qc_test == c("rolling_sd"), variable == "temperature_degree_c") %>%
  rbind(newrow_sd)

```

::: panel-tabset
```{r}
#| warning: false
#| message: false
#| echo: false
#| results: asis
#| fig-height: 5


for (i in list) {
  dat_i <- dat_all %>%
    filter(station == i)
  
cat(paste("### ", i, "\n \n"))
  
#apply test
qc_i <- qc_test_rolling_sd(dat_i, county = "river_data")

#plot
qc2_i <- qc_pivot_longer(qc_i, qc_tests = "rolling_sd")
p <- qc_plot_flags(qc2_i, qc_tests = "rolling_sd", var = "temperature_degree_c")
print(p$temperature_degree_c$rolling_sd)


cat("\n\n")
}

```

::: 


